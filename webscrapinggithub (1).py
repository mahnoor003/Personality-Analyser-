# -*- coding: utf-8 -*-
"""webscrapinggithub.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-r3Qp_4SFiOQ7hcQOn5S69DbsUKsvAq8
"""

import requests
import base64
import time
import json
import csv

# GitHub authentication
GITHUB_TOKEN = ''  # Replace with your token
HEADERS = {
    'Authorization': f'token {GITHUB_TOKEN}',
    'Accept': 'application/vnd.github+json'
}

# Search GitHub users by location
def search_users_by_location(location='Pakistan', per_page=30):
    url = f'https://api.github.com/search/users?q=location:{location}&per_page={per_page}'
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    return response.json().get('items', [])

# Get basic user profile
def get_user_details(username):
    url = f'https://api.github.com/users/{username}'
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    return response.json()

# Get user repositories
def get_user_repos(username):
    url = f'https://api.github.com/users/{username}/repos?per_page=100&sort=updated'
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    return response.json()

# Get the latest commit message from a repository
def get_latest_commit(username, repo_name):
    url = f'https://api.github.com/repos/{username}/{repo_name}/commits?per_page=1'
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        commits = response.json()
        if isinstance(commits, list) and commits:
            return commits[0]['commit']['message']
    return None

# Get programming languages used in a repository
def get_languages(username, repo_name):
    url = f'https://api.github.com/repos/{username}/{repo_name}/languages'
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        return list(response.json().keys())
    return []

# Get README content for a repository
def get_readme(username, repo_name):
    url = f'https://api.github.com/repos/{username}/{repo_name}/readme'
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        content = response.json().get('content')
        if content:
            try:
                return base64.b64decode(content).decode('utf-8', errors='ignore')
            except Exception:
                return "Error decoding README"
    return None

# Collect complete profile data
def collect_data(users):
    profiles = []

    for user in users:
        username = user['login']
        print(f'üîç Collecting data for {username}...')
        try:
            details = get_user_details(username)
            repos = get_user_repos(username)
        except Exception as e:
            print(f"‚ùå Error fetching data for {username}: {e}")
            continue

        user_repos = []
        for repo in repos[:3]:  # Limit to 3 repos per user to avoid rate limits
            repo_name = repo['name']
            description = repo.get('description')
            commit_msg = get_latest_commit(username, repo_name)
            languages = get_languages(username, repo_name)
            readme = get_readme(username, repo_name)

            user_repos.append({
                'repo_name': repo_name,
                'description': description,
                'languages': languages,
                'latest_commit': commit_msg,
                'readme': readme
            })
            time.sleep(1)  # To respect API rate limits

        profiles.append({
            'username': username,
            'name': details.get('name'),
            'followers': details.get('followers'),
            'following': details.get('following'),
            'public_repos': details.get('public_repos'),
            'repos': user_repos
        })
        time.sleep(1)

    return profiles

# Main execution
if __name__ == '__main__':
    try:
        users = search_users_by_location('Pakistan', per_page=30)
        results = collect_data(users)

        # Save to JSON
        with open('github_profiles_pakistan.json', 'w', encoding='utf-8') as f_json:
            json.dump(results, f_json, ensure_ascii=False, indent=4)
        print("‚úÖ Data saved to 'github_profiles_pakistan.json'")

        # Save to CSV
        with open('github_profiles_pakistan.csv', 'w', newline='', encoding='utf-8') as f_csv:
            writer = csv.writer(f_csv)
            writer.writerow(['Username', 'Name', 'Followers', 'Following', 'Public Repos',
                             'Repo Name', 'Description', 'Languages', 'Latest Commit', 'README'])

            for profile in results:
                for repo in profile['repos']:
                    writer.writerow([
                        profile['username'],
                        profile.get('name', ''),
                        profile.get('followers', ''),
                        profile.get('following', ''),
                        profile.get('public_repos', ''),
                        repo.get('repo_name', ''),
                        repo.get('description', ''),
                        ', '.join(repo.get('languages', [])),
                        repo.get('latest_commit', ''),
                        (repo['readme'][:100] + '...') if repo['readme'] else 'No README'
                    ])
        print("‚úÖ Data saved to 'github_profiles_pakistan.csv'")

    except Exception as e:
        print(f"‚ùå Fatal error: {e}")

import requests
import base64
import time
import json
import csv

# GitHub authentication
GITHUB_TOKEN = ''  # Replace with your token
HEADERS = {
    'Authorization': f'token {GITHUB_TOKEN}',
    'Accept': 'application/vnd.github+json'
}

# Search GitHub users by location with pagination
def search_users_by_location(location='Pakistan', per_page=30, page=1):
    url = f'https://api.github.com/search/users?q=location:{location}&per_page={per_page}&page={page}'
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    return response.json().get('items', [])

# Get basic user profile
def get_user_details(username):
    url = f'https://api.github.com/users/{username}'
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    return response.json()

# Get user repositories
def get_user_repos(username):
    url = f'https://api.github.com/users/{username}/repos?per_page=100&sort=updated'
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    return response.json()

# Get the latest commit message from a repository
def get_latest_commit(username, repo_name):
    url = f'https://api.github.com/repos/{username}/{repo_name}/commits?per_page=1'
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        commits = response.json()
        if isinstance(commits, list) and commits:
            return commits[0]['commit']['message']
    return None

# Get programming languages used in a repository
def get_languages(username, repo_name):
    url = f'https://api.github.com/repos/{username}/{repo_name}/languages'
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        return list(response.json().keys())
    return []

# Get README content for a repository
def get_readme(username, repo_name):
    url = f'https://api.github.com/repos/{username}/{repo_name}/readme'
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        content = response.json().get('content')
        if content:
            try:
                return base64.b64decode(content).decode('utf-8', errors='ignore')
            except Exception:
                return "Error decoding README"
    return None

# Collect complete profile data
def collect_data(users):
    profiles = []
    seen_usernames = set()

    for user in users:
        username = user['login']
        if username in seen_usernames:
            continue
        seen_usernames.add(username)

        print(f'üîç Collecting data for {username}...')
        try:
            details = get_user_details(username)
            repos = get_user_repos(username)
        except Exception as e:
            print(f"‚ùå Error fetching data for {username}: {e}")
            continue

        user_repos = []
        for repo in repos[:3]:  # Limit to 3 repos per user to avoid rate limits
            repo_name = repo['name']
            description = repo.get('description')
            commit_msg = get_latest_commit(username, repo_name)
            languages = get_languages(username, repo_name)
            readme = get_readme(username, repo_name)

            user_repos.append({
                'repo_name': repo_name,
                'description': description,
                'languages': languages,
                'latest_commit': commit_msg,
                'readme': readme
            })
            time.sleep(1)  # To respect API rate limits

        profiles.append({
            'username': username,
            'name': details.get('name'),
            'followers': details.get('followers'),
            'following': details.get('following'),
            'public_repos': details.get('public_repos'),
            'repos': user_repos
        })
        time.sleep(1)

    return profiles

if __name__ == '__main__':
    try:
        # Load existing usernames
        existing_usernames = set()
        existing_data = []
        try:
            with open('github_profiles_pakistan.json', 'r', encoding='utf-8') as f_json:
                existing_data = json.load(f_json)
                existing_usernames = {user['username'] for user in existing_data}
            print(f"üìÅ Loaded {len(existing_usernames)} existing users.")
        except FileNotFoundError:
            print("üìÇ No existing data found. Starting fresh.")

        all_users = []
        total_pages = 5  # You can adjust this to fetch more users

        for page in range(1, total_pages + 1):
            print(f'üìÑ Fetching users from page {page}...')
            page_users = search_users_by_location('Pakistan', per_page=30, page=page)
            for user in page_users:
                if user['login'] not in existing_usernames:
                    all_users.append(user)
            time.sleep(2)

        if not all_users:
            print("‚úÖ No new users found.")
            exit()

        print(f"üÜï {len(all_users)} new users found.")

        new_results = collect_data(all_users)

        # Append to JSON
        with open('github_profiles_pakistan.json', 'w', encoding='utf-8') as f_json:
            updated_data = existing_data + new_results
            json.dump(updated_data, f_json, ensure_ascii=False, indent=4)
        print("‚úÖ JSON file updated with new users.")

        # Append to CSV
        with open('github_profiles_pakistan.csv', 'a', newline='', encoding='utf-8') as f_csv:
            writer = csv.writer(f_csv)
            for profile in new_results:
                for repo in profile['repos']:
                    writer.writerow([
                        profile['username'],
                        profile.get('name', ''),
                        profile.get('followers', ''),
                        profile.get('following', ''),
                        profile.get('public_repos', ''),
                        repo.get('repo_name', ''),
                        repo.get('description', ''),
                        ', '.join(repo.get('languages', [])),
                        repo.get('latest_commit', ''),
                        (repo['readme'][:100] + '...') if repo['readme'] else 'No README'
                    ])
        print("‚úÖ CSV file updated with new users.")

    except Exception as e:
        print(f"‚ùå Fatal error: {e}")

df.info()

import pandas as pd
import re

# Load CSV
df = pd.read_csv("/content/github_profiles_pakistan.csv")

df.info()